{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad536149-5362-4d91-87fd-3150fc82e891",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "792abca7-f4ea-4f3d-bf4a-cfb73b3d8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.utils import np_utils, generic_utils\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "663fc766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb  8 17:25:26 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.23       Driver Version: 511.23       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 44%   54C    P2   103W / 250W |   9823MiB / 11264MiB |      5%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      5960      C   ...a3\\envs\\tf_gpu\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56be23ed-a002-4a66-a476-64467b9f571d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s1', 's11', 's12', 's13', 's14', 's15', 's18', 's19', 's2', 's20', 's3', 's4', 's5', 's6', 's8', 's9']\n"
     ]
    }
   ],
   "source": [
    "#Number of subjects\n",
    "print(os.listdir(subjectPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ffd565-03c7-4028-92b2-664439642c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rows, image_columns, frames_Count = 112, 112, 16\n",
    "training_smtc_list_netA = []\n",
    "training_smtc_list_netB = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a1fcbb-9736-4100-9836-0ec25b1f53e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectPath = 'Datasets/SMIC/HS'\n",
    "#subDirectory = os.listdir(subjectPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0290a5-5ce7-4ea7-9f49-d29af93ef4de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Converting into arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce1a4935-2a70-4f36-bf45-095e17bb7dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_image559741.bmp\n"
     ]
    }
   ],
   "source": [
    "for sub in subDirectory:\n",
    "    framespath = 'Datasets/SMIC/HS/' + sub + '/micro/negative/'\n",
    "    frames = os.listdir(framespath)\n",
    "    #print(frames)\n",
    "    \n",
    "    for parts in frames:\n",
    "        fm = os.listdir(framespath + parts)\n",
    "        print(fm[1])\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5cf12-80a8-429f-ac43-4725c834d6d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SMIC Datasset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078510aa-4603-41b8-8c14-934763eded4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada7ca5f-def5-4971-956f-b714e54cf848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative 69\n",
      "positive 50\n",
      "surprise 43\n"
     ]
    }
   ],
   "source": [
    "#Dataset has three different folders of micro-expressions\n",
    "for lab in ['negative', 'positive', 'surprise']:\n",
    "    subDirectory = os.listdir(subjectPath)\n",
    "    count = 0    \n",
    "    #From each fetching the subjects/persons\n",
    "    for sub in subDirectory:\n",
    "        labelpath = 'Datasets/SMIC/HS/'+ sub +'/micro/' + lab + '/'\n",
    "        directorylisting = os.listdir(labelpath)\n",
    "        #Each subject has multiple expression video files\n",
    "        for video in directorylisting:\n",
    "            videopath = labelpath + video\n",
    "            frames = []\n",
    "            framelisting = os.listdir(videopath)\n",
    "            #Filtering the subjects having the frames than the frame count to avoid dimensionality problem \n",
    "            if len(framelisting) > frames_Count:\n",
    "                framerange = [x for x in range(frames_Count)]\n",
    "                count = count + 1\n",
    "                #Pre-process every single frame\n",
    "                for frame in framerange:\n",
    "                    imagepath = videopath + \"/\" + framelisting[frame]\n",
    "                    image = cv2.imread(imagepath)\n",
    "                    imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)\n",
    "                    # grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)\n",
    "                    frames.append(imageresize)\n",
    "                    # break\n",
    "                frames = np.asarray(frames)\n",
    "                videoarray = np.rollaxis(np.rollaxis(frames, 2, 0), 2, 0)\n",
    "                training_smtc_list_netA.append(videoarray)\n",
    "              # break\n",
    "    #Video samples count for each expressions\n",
    "    print(lab,count)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06472f2a-a620-4445-89b2-662eb851a1f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smtc_train_netA = np.asarray(training_smtc_list_netA)\n",
    "smtc_train_netA = np.rollaxis(np.rollaxis(smtc_train_netA, 2, 4), 2, 1)\n",
    "smtc_train_netA = (smtc_train_netA.astype('float32') / 255)\n",
    "\n",
    "# Assign Labels\n",
    "smtc_target_netA = np.zeros((len(smtc_train_netA), ), dtype = int)\n",
    "smtc_target_netA[0:69] = 0\n",
    "smtc_target_netA[69:119] = 1\n",
    "smtc_target_netA[119:162] = 2\n",
    "\n",
    "# Convert to one hot\n",
    "# smtc_target = np_utils.to_categorical(smtc_target, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c982c7fb-70e4-4134-aa3a-9d5380f2cfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 16, 112, 112, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of sample, number of frames from each video, hieght, width, channels\n",
    "smtc_train_netA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f19ee9f-f9e8-4d07-a915-a535be5135f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Nump Arrays to save time\n",
    "np.save('training_datasets/microexp_2D_to_3D_imagesA.npy', smtc_train_netA)\n",
    "np.save('training_datasets/microexp_2D_to_3D_labelsA.npy', smtc_target_netA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73892c66-84cf-4a02-b219-a9b0e562ae91",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ef979f",
   "metadata": {},
   "source": [
    "Model-B pre-processing is similar flow as model-A and obtaining the differential image(colarating all the images difference into single image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05b172c5-162e-4d54-ba72-a5e4ce96c75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative 69\n",
      "positive 50\n",
      "surprise 43\n"
     ]
    }
   ],
   "source": [
    "training_smtc_list_netB = []\n",
    "for lab in ['negative', 'positive', 'surprise']:\n",
    "    subDirectory = os.listdir(subjectPath)\n",
    "    count = 0\n",
    "    for sub in subDirectory:\n",
    "        labelpath = 'Datasets/SMIC/HS/'+ sub +'/micro/' + lab + '/'\n",
    "        directorylisting = os.listdir(labelpath)\n",
    "        for video in directorylisting:\n",
    "            videopath = labelpath + video\n",
    "            frames = []\n",
    "            framelisting = os.listdir(videopath)\n",
    "            if len(framelisting) > frames_Count:\n",
    "                randomStartFrame = randrange(len(framelisting)-frames_Count)\n",
    "                framerange = [x for x in range(randomStartFrame, (randomStartFrame+frames_Count))]\n",
    "                count = count + 1\n",
    "                for frame in framerange:\n",
    "                    imagepath = videopath + \"/\" + framelisting[frame]\n",
    "                    image = cv2.imread(imagepath)\n",
    "                    imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)\n",
    "                    # grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)\n",
    "                    frames.append(imageresize)\n",
    "                frames = np.asarray(frames)\n",
    "                diff_info_Image = np.zeros(shape=[112,112,3])\n",
    "                for indCount, images in enumerate(frames):\n",
    "                    if indCount == 0:\n",
    "                        #Do Nothing\n",
    "                        continue\n",
    "                    diff_info_Image = diff_info_Image + (np.absolute(images-frames[0]))\n",
    "              # videoarray = np.rollaxis(np.rollaxis(frames, 2, 0), 2, 0)\n",
    "                training_smtc_list_netB.append(diff_info_Image)\n",
    "    print(lab,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3027153a-bcb6-4be6-938f-e6a55eb8012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "smtc_train_netB = np.asarray(training_smtc_list_netB)\n",
    "# smtc_train_netB = (smtc_train_netB.astype('float32') / 255)\n",
    "\n",
    "# Assign Labels\n",
    "smtc_target_netB = np.zeros((len(smtc_train_netB), ), dtype = int)\n",
    "smtc_target_netB[0:69] = 0\n",
    "smtc_target_netB[66:119] = 1\n",
    "smtc_target_netB[113:162] = 2\n",
    "\n",
    "# Convert to one hot\n",
    "# smtc_target = np_utils.to_categorical(smtc_target, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6c72449-61fb-4987-9104-47d82acdcedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 112, 112, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of sample, hieght, width, channels\n",
    "smtc_train_netB.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1790f34-d86a-4612-8b68-a81c9aef7e60",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f852e78e-b02e-494a-b0f6-f18ffaf3a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('training_datasets/microexp_2D_to_3D_imagesB.npy', smtc_train_netB)\n",
    "np.save('training_datasets/microexp_2D_to_3D_labelsB.npy', smtc_target_netB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8236633-8f49-4cff-91b2-6b5c4ff756e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3785782b-0e55-4fbb-a9b0-716d4aaf28b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 16, 112, 112, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the saved numpy data NetA\n",
    "smtc_train_netA = None\n",
    "smtc_target_netA = None\n",
    "smtc_train_netA = np.load('training_datasets/microexp_2D_to_3D_imagesA.npy')\n",
    "smtc_target_netA =np.load('training_datasets/microexp_2D_to_3D_labelsA.npy')\n",
    "\n",
    "smtc_train_netA.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e462889-3e07-477f-88b2-2a11e6f2e4f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3f5474-8553-42f9-9e8b-51033f26e5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 112, 112, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the saved numpy data NetB\n",
    "smtc_train_netB = None\n",
    "smtc_target_netB = None\n",
    "smtc_train_netB = np.load('training_datasets/microexp_2D_to_3D_imagesB.npy')\n",
    "smtc_target_netB =np.load('training_datasets/microexp_2D_to_3D_labelsB.npy')\n",
    "\n",
    "smtc_train_netB.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f901ffa-4650-4b85-89a8-684a1933e796",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c86179-2f3c-4efc-9644-4ce56cca782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_A, x_test_A, x_train_B, x_test_B, y_train, y_test = train_test_split(smtc_train_netA, smtc_train_netB, smtc_target_netA, test_size=0.2, random_state=4)\n",
    "\n",
    "y_train_A = y_train\n",
    "y_test_A = y_test\n",
    "\n",
    "y_train_B = y_train\n",
    "y_test_B = y_test\n",
    "\n",
    "\n",
    "# Convert Into Tensors\n",
    "train_data_A = tf.data.Dataset.from_tensor_slices((x_train_A, y_train_A)).batch(8, drop_remainder=True)\n",
    "test_data_A = tf.data.Dataset.from_tensor_slices((x_test_A, y_test_A)).batch(8, drop_remainder=True)\n",
    "\n",
    "train_data_B = tf.data.Dataset.from_tensor_slices((x_train_B, y_train_B)).batch(8, drop_remainder=True)\n",
    "test_data_B = tf.data.Dataset.from_tensor_slices((x_test_B, y_test_B)).batch(8, drop_remainder=True)\n",
    "\n",
    "# Use the below dataset for model.fit\n",
    "train_All_dataset = tf.data.Dataset.from_tensor_slices(({\"NET_A_InputLayer\": x_train_A, \"NET_B_InputLayer\": x_train_B}, y_train_A)).batch(8, drop_remainder=True)\n",
    "test_All_dataset = tf.data.Dataset.from_tensor_slices(({\"NET_A_InputLayer\": x_test_A, \"NET_B_InputLayer\": x_test_B}, y_test_A)).batch(8, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b4809-b428-47b8-b0fb-04db228a97cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Architecture of Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50812bb2-ebe1-4a3d-b690-8616dfe6b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _modelA(name=\"NET_A1\"):\n",
    "    input_shape = (16, 112, 112, 3)\n",
    "\n",
    "  \n",
    "    input = tf.keras.Input(shape=input_shape, name=\"NET_A_InputLayer\")\n",
    "\n",
    "      # Block 1\n",
    "      # Convolution Layers\n",
    "     \n",
    "    conv_layer_1_1 = tf.keras.layers.Conv3D(filters=16,kernel_size=(1,3,3), strides=(1,2,2), padding=\"same\", name=\"Conv3d_layer1_1\")(input) \n",
    "    conv_layer_1_2 = tf.keras.layers.Conv3D(filters=16,kernel_size=(3,1,1), strides=(1,2,2), padding=\"same\", name=\"Conv3d_layer1_2\")(input)\n",
    "    conv_layer_1_3 = tf.keras.layers.Conv3D(filters=16,kernel_size=(5,1,1), strides=(1,2,2), padding=\"same\", name=\"Conv3d_layer1_3\")(input)\n",
    "\n",
    "\n",
    "      # Concatenation Layer\n",
    "    concat_layer_1 = tf.keras.layers.concatenate([conv_layer_1_1, conv_layer_1_2, conv_layer_1_3], axis=1)\n",
    "\n",
    "\n",
    "      # Block 2\n",
    "    conv_layer_2_1 = tf.keras.layers.Conv3D(filters=16,kernel_size=(1,3,3), strides=(1,2,2), padding=\"same\", name=\"Conv3d_layer2_1\")(concat_layer_1)\n",
    "    conv_layer_2_2 = tf.keras.layers.Conv3D(filters=16,kernel_size=(3,1,1), strides=(1,2,2), padding=\"same\", name=\"Conv3d_layer2_2\")(concat_layer_1)\n",
    "    conv_layer_2_3 = tf.keras.layers.Conv3D(filters=16,kernel_size=(5,1,1), strides=(1,2,2), padding=\"same\", name=\"Conv3d_layer2_3\")(concat_layer_1)\n",
    "\n",
    "      #Max Pool Layers\n",
    "    conv_maxpool_layer_2_2 = tf.keras.layers.MaxPool3D(pool_size=(2, 1, 1), name=\"Conv3d_maxpool_layer2_2\")(conv_layer_2_2)\n",
    "    conv_maxpool_layer_2_3 = tf.keras.layers.MaxPool3D(pool_size=(2, 1, 1), name=\"Conv3d_maxpool_layer2_3\")(conv_layer_2_3)\n",
    "\n",
    "    concat_layer_2 = tf.keras.layers.concatenate([conv_layer_2_1, conv_maxpool_layer_2_2, conv_maxpool_layer_2_3], axis=1)\n",
    "\n",
    "      # Block 3\n",
    "    conv_layer_3_1 = tf.keras.layers.Conv3D(filters=32,kernel_size=(1,3,3), strides=(1,2,2), padding=\"same\", name=\"Conv3d_layer3_1\")(concat_layer_2)\n",
    "    conv_layer_3_2 = tf.keras.layers.Conv3D(filters=32,kernel_size=(3,1,1), strides=(1,2,2), padding=\"same\", name=\"Conv3d_layer3_2\")(concat_layer_2)\n",
    "    conv_layer_3_3 = tf.keras.layers.Conv3D(filters=32,kernel_size=(5,1,1), strides=(1,2,2), padding=\"same\", name=\"Conv3d_layer3_3\")(concat_layer_2)\n",
    "\n",
    "    conv_maxpool_layer_3_2 = tf.keras.layers.MaxPool3D(pool_size=(2, 1, 1), name=\"Conv3d_maxpool_layer3_2\")(conv_layer_3_2)\n",
    "    conv_maxpool_layer_3_3 = tf.keras.layers.MaxPool3D(pool_size=(2, 1, 1), name=\"Conv3d_maxpool_layer3_3\")(conv_layer_3_3)\n",
    "\n",
    "    concat_layer_3 = tf.keras.layers.concatenate([conv_layer_3_1, conv_maxpool_layer_3_2, conv_maxpool_layer_3_3], axis=1)\n",
    "\n",
    "      # Block 4\n",
    "    conv_layer_4_1 = tf.keras.layers.Conv3D(filters=64,kernel_size=(1,3,3), strides=(1,2,2), padding=\"same\", name=\"Conv3d_layer4_1\")(concat_layer_3)\n",
    "    conv_layer_4_2 = tf.keras.layers.Conv3D(filters=64,kernel_size=(3,1,1), strides=(1,2,2), padding=\"same\", name=\"Conv3d_layer4_2\")(concat_layer_3)\n",
    "    conv_layer_4_3 = tf.keras.layers.Conv3D(filters=64,kernel_size=(5,1,1), strides=(1,2,2), padding=\"same\", name=\"Conv3d_layer4_3\")(concat_layer_3)\n",
    "\n",
    "    conv_maxpool_layer_4_2 = tf.keras.layers.MaxPool3D(pool_size=(2, 1, 1), name=\"Conv3d_maxpool_layer4_2\")(conv_layer_4_2)\n",
    "    conv_maxpool_layer_4_3 = tf.keras.layers.MaxPool3D(pool_size=(2, 1, 1), name=\"Conv3d_maxpool_layer4_3\")(conv_layer_4_3)\n",
    "\n",
    "    concat_layer_4 = tf.keras.layers.concatenate([conv_layer_4_1, conv_maxpool_layer_4_2, conv_maxpool_layer_4_3], axis=1)\n",
    "\n",
    "      # Block 5\n",
    "    conv_layer_5 = tf.keras.layers.Conv3D(filters=128,kernel_size=(3,3,3), padding=\"same\", name=\"Conv3d_layer5\")(concat_layer_4)\n",
    "    conv_maxpool_layer_5 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding=\"same\", name=\"Conv3d_maxpool_layer5\")(conv_layer_5)\n",
    "\n",
    "    func_model = tf.keras.Model(input, conv_maxpool_layer_5, name=name)\n",
    "\n",
    "    return func_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "763182e2-6982-4d12-9916-e40db5ae07ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = _modelA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8605db41-bc1d-4a05-9fb5-601036460081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# model_A.summary()\n",
    "tf.keras.utils.plot_model(model_A, \"NETA.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22c6af31-03c5-4455-8e04-e8da3faef5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ec102-20fe-4c53-8928-55e009559020",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Architecture of Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0278392d-ca79-4d98-b6b3-0c945ac53835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _modelB(name=\"NET_B1\"):\n",
    "    input_shape = (112, 112, 3)\n",
    "\n",
    "\n",
    "    input = tf.keras.Input(shape=input_shape, name=\"NET_B_InputLayer\")\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters=16,kernel_size=(3,3), padding=\"same\", name=\"Res_Layer1\")(input)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x_skip = x\n",
    "\n",
    "      # Residual Block 1\n",
    "    x = tf.keras.layers.Conv2D(filters=16,kernel_size=(3,3), padding=\"same\", name=\"Res_Block_Layer1_1\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=16,kernel_size=(3,3), padding=\"same\", name=\"Res_Block_Layer1_2\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Add()([x, x_skip])\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(pool_size=(2, 2), name=\"Layer1_Avg_Pool\")(x)\n",
    "    x_skip = x\n",
    "\n",
    "\n",
    "      # Residual Block 2\n",
    "    x = tf.keras.layers.Conv2D(filters=16,kernel_size=(3,3), padding=\"same\", name=\"Res_Block_Layer2_1\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=16,kernel_size=(3,3), padding=\"same\", name=\"Res_Block_Layer2_2\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Add()([x, x_skip])\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(pool_size=(2, 2), name=\"Layer2_Avg_Pool\")(x)\n",
    "\n",
    "    x_skip = tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3), padding=\"same\", name=\"Res_Layer2\")(x)\n",
    "\n",
    "      # Residual Block 3\n",
    "    x = tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3), padding=\"same\", name=\"Res_Block_Layer3_1\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3), padding=\"same\", name=\"Res_Block_Layer3_2\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Add()([x, x_skip])\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(pool_size=(2, 2), name=\"Layer3_Avg_Pool\")(x)\n",
    "\n",
    "\n",
    "    x_skip = tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3), padding=\"same\", name=\"Res_Layer3\")(x)\n",
    "\n",
    "      # Residual Block 4\n",
    "    x = tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3), padding=\"same\", name=\"Res_Block_Layer4_1\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3), padding=\"same\", name=\"Res_Block_Layer4_2\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Add()([x, x_skip])\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(pool_size=(2, 2), name=\"Layer4_Avg_Pool\")(x)\n",
    "\n",
    "    x_skip = tf.keras.layers.Conv2D(filters=128,kernel_size=(3,3), padding=\"same\", name=\"Res_Layer4\")(x)\n",
    "\n",
    "      # Residual Block 5\n",
    "    x = tf.keras.layers.Conv2D(filters=128,kernel_size=(3,3), padding=\"same\", name=\"Res_Block_Layer5_1\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=128,kernel_size=(3,3), padding=\"same\", name=\"Res_Block_Layer5_2\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Add()([x, x_skip])\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(pool_size=(2, 2), padding=\"same\", name=\"Layer5_Avg_Pool\")(x)\n",
    "\n",
    "\n",
    "    func_model = tf.keras.Model(input, x, name=name)\n",
    "\n",
    "    return func_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "987f6211-c870-4eaf-ac59-b939f408d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = _modelB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59760b65-917b-4948-86ce-a531ee2990a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# model_A.summary()\n",
    "tf.keras.utils.plot_model(model_B, \"NETB.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0172789c-3580-42e1-8611-d31cd056398c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Concat the Architecture of Model A & Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "442a1492-0355-4cf5-8e20-1e36302205ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _modelAB(name=\"NET_AB\"):\n",
    "    #NetA\n",
    "    NetAInput = model_A.input\n",
    "    NetA_Output = model_A.layers[-1].output\n",
    "\n",
    "      #NetB\n",
    "    NetBInput = model_B.input\n",
    "    NetB_Output = model_B.layers[-1].output\n",
    "\n",
    "      #Add\n",
    "    added = tf.keras.layers.Add(name=\"Add\")([NetA_Output, NetB_Output])\n",
    "\n",
    "    flatten = tf.keras.layers.Flatten(name=\"Flatten\")(added)\n",
    "\n",
    "    full_Connected = tf.keras.layers.Dense(units=3, name=\"Fully_Connected\")(flatten)\n",
    "\n",
    "    func_model = tf.keras.Model([NetAInput, NetBInput], full_Connected, name=name)\n",
    "\n",
    "    return func_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5429e15a-e823-4067-a2e0-160efdf109c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_AB = _modelAB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a66b762-5579-4386-9c76-da73006b7cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model_AB, \"NETAB.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c3c91-c3a4-48ea-827e-5d9f7622a7ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57beef22-444a-41c5-baa7-e6831c855546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1\n",
      "Step: 0 Loss: 207.6307830810547 Accuracy: 0.25\n",
      "Step: 4 Loss: 772.58544921875 Accuracy: 0.21875\n",
      "Step: 8 Loss: 122.43148803710938 Accuracy: 0.375\n",
      "Step: 12 Loss: 34.019657135009766 Accuracy: 0.4375\n",
      "Epoch :  2\n",
      "Step: 0 Loss: 40.51036071777344 Accuracy: 0.4375\n",
      "Step: 4 Loss: 26.190982818603516 Accuracy: 0.15625\n",
      "Step: 8 Loss: 7.31225061416626 Accuracy: 0.28125\n",
      "Step: 12 Loss: 2.9041051864624023 Accuracy: 0.3125\n",
      "Epoch :  3\n",
      "Step: 0 Loss: 1.249194622039795 Accuracy: 0.4375\n",
      "Step: 4 Loss: 1.8004484176635742 Accuracy: 0.375\n",
      "Step: 8 Loss: 1.339045763015747 Accuracy: 0.40625\n",
      "Step: 12 Loss: 1.629081130027771 Accuracy: 0.3125\n",
      "Epoch :  4\n",
      "Step: 0 Loss: 0.9721174240112305 Accuracy: 0.46875\n",
      "Step: 4 Loss: 0.7025880813598633 Accuracy: 0.375\n",
      "Step: 8 Loss: 0.897973895072937 Accuracy: 0.53125\n",
      "Step: 12 Loss: 1.3413565158843994 Accuracy: 0.3125\n",
      "Epoch :  5\n",
      "Step: 0 Loss: 1.1992677450180054 Accuracy: 0.46875\n",
      "Step: 4 Loss: 0.5311616659164429 Accuracy: 0.53125\n",
      "Step: 8 Loss: 0.8291283249855042 Accuracy: 0.53125\n",
      "Step: 12 Loss: 1.4228256940841675 Accuracy: 0.40625\n",
      "Epoch :  6\n",
      "Step: 0 Loss: 0.9728822708129883 Accuracy: 0.40625\n",
      "Step: 4 Loss: 0.6324506998062134 Accuracy: 0.59375\n",
      "Step: 8 Loss: 0.8881438970565796 Accuracy: 0.4375\n",
      "Step: 12 Loss: 1.5720592737197876 Accuracy: 0.375\n",
      "Epoch :  7\n",
      "Step: 0 Loss: 1.1147732734680176 Accuracy: 0.34375\n",
      "Step: 4 Loss: 0.5481727719306946 Accuracy: 0.625\n",
      "Step: 8 Loss: 1.1210224628448486 Accuracy: 0.40625\n",
      "Step: 12 Loss: 1.3150203227996826 Accuracy: 0.5\n",
      "Epoch :  8\n",
      "Step: 0 Loss: 1.216354489326477 Accuracy: 0.3125\n",
      "Step: 4 Loss: 0.5972241759300232 Accuracy: 0.75\n",
      "Step: 8 Loss: 1.1590523719787598 Accuracy: 0.375\n",
      "Step: 12 Loss: 1.2914097309112549 Accuracy: 0.46875\n",
      "Epoch :  9\n",
      "Step: 0 Loss: 1.0986948013305664 Accuracy: 0.46875\n",
      "Step: 4 Loss: 0.454997718334198 Accuracy: 0.75\n",
      "Step: 8 Loss: 0.7888041734695435 Accuracy: 0.53125\n",
      "Step: 12 Loss: 1.2587804794311523 Accuracy: 0.46875\n",
      "Epoch :  10\n",
      "Step: 0 Loss: 0.8768994808197021 Accuracy: 0.59375\n",
      "Step: 4 Loss: 0.4975395202636719 Accuracy: 0.71875\n",
      "Step: 8 Loss: 0.8104874491691589 Accuracy: 0.5\n",
      "Step: 12 Loss: 1.1128212213516235 Accuracy: 0.625\n",
      "Epoch :  11\n",
      "Step: 0 Loss: 1.113738775253296 Accuracy: 0.4375\n",
      "Step: 4 Loss: 0.49603763222694397 Accuracy: 0.78125\n",
      "Step: 8 Loss: 0.9066025614738464 Accuracy: 0.53125\n",
      "Step: 12 Loss: 1.1036436557769775 Accuracy: 0.59375\n",
      "Epoch :  12\n",
      "Step: 0 Loss: 0.9420738220214844 Accuracy: 0.4375\n",
      "Step: 4 Loss: 0.47033947706222534 Accuracy: 0.75\n",
      "Step: 8 Loss: 0.7302502989768982 Accuracy: 0.5\n",
      "Step: 12 Loss: 1.1543288230895996 Accuracy: 0.59375\n",
      "Epoch :  13\n",
      "Step: 0 Loss: 0.940735936164856 Accuracy: 0.625\n",
      "Step: 4 Loss: 0.4282378554344177 Accuracy: 0.78125\n",
      "Step: 8 Loss: 0.7642683982849121 Accuracy: 0.625\n",
      "Step: 12 Loss: 1.0055433511734009 Accuracy: 0.59375\n",
      "Epoch :  14\n",
      "Step: 0 Loss: 1.0573458671569824 Accuracy: 0.46875\n",
      "Step: 4 Loss: 0.4632374942302704 Accuracy: 0.78125\n",
      "Step: 8 Loss: 0.8150891065597534 Accuracy: 0.625\n",
      "Step: 12 Loss: 0.9559571743011475 Accuracy: 0.59375\n",
      "Epoch :  15\n",
      "Step: 0 Loss: 1.0122478008270264 Accuracy: 0.5625\n",
      "Step: 4 Loss: 0.4285591244697571 Accuracy: 0.78125\n",
      "Step: 8 Loss: 0.7219135761260986 Accuracy: 0.625\n",
      "Step: 12 Loss: 0.9631751179695129 Accuracy: 0.6875\n",
      "Epoch :  16\n",
      "Step: 0 Loss: 0.9739850759506226 Accuracy: 0.59375\n",
      "Step: 4 Loss: 0.4340720772743225 Accuracy: 0.78125\n",
      "Step: 8 Loss: 0.7242320775985718 Accuracy: 0.65625\n",
      "Step: 12 Loss: 0.8664261102676392 Accuracy: 0.71875\n",
      "Epoch :  17\n",
      "Step: 0 Loss: 1.0220526456832886 Accuracy: 0.625\n",
      "Step: 4 Loss: 0.46270573139190674 Accuracy: 0.78125\n",
      "Step: 8 Loss: 0.6929894089698792 Accuracy: 0.6875\n",
      "Step: 12 Loss: 0.8873856663703918 Accuracy: 0.6875\n",
      "Epoch :  18\n",
      "Step: 0 Loss: 0.8319616317749023 Accuracy: 0.71875\n",
      "Step: 4 Loss: 0.4016088545322418 Accuracy: 0.875\n",
      "Step: 8 Loss: 0.6251018047332764 Accuracy: 0.75\n",
      "Step: 12 Loss: 0.7535322904586792 Accuracy: 0.75\n",
      "Epoch :  19\n",
      "Step: 0 Loss: 1.1308506727218628 Accuracy: 0.625\n",
      "Step: 4 Loss: 0.40642914175987244 Accuracy: 0.8125\n",
      "Step: 8 Loss: 0.7049540281295776 Accuracy: 0.71875\n",
      "Step: 12 Loss: 0.6610130667686462 Accuracy: 0.71875\n",
      "Epoch :  20\n",
      "Step: 0 Loss: 1.1628706455230713 Accuracy: 0.59375\n",
      "Step: 4 Loss: 0.5892390012741089 Accuracy: 0.78125\n",
      "Step: 8 Loss: 0.7093827724456787 Accuracy: 0.75\n",
      "Step: 12 Loss: 0.6749368906021118 Accuracy: 0.78125\n",
      "Epoch :  21\n",
      "Step: 0 Loss: 1.0563799142837524 Accuracy: 0.71875\n",
      "Step: 4 Loss: 0.606286883354187 Accuracy: 0.78125\n",
      "Step: 8 Loss: 0.5976221561431885 Accuracy: 0.8125\n",
      "Step: 12 Loss: 0.6499305963516235 Accuracy: 0.75\n",
      "Epoch :  22\n",
      "Step: 0 Loss: 0.8066344261169434 Accuracy: 0.78125\n",
      "Step: 4 Loss: 0.8185465335845947 Accuracy: 0.71875\n",
      "Step: 8 Loss: 0.40498077869415283 Accuracy: 0.8125\n",
      "Step: 12 Loss: 0.6688793897628784 Accuracy: 0.75\n",
      "Epoch :  23\n",
      "Step: 0 Loss: 0.5484741926193237 Accuracy: 0.84375\n",
      "Step: 4 Loss: 0.872132420539856 Accuracy: 0.75\n",
      "Step: 8 Loss: 0.23506039381027222 Accuracy: 0.875\n",
      "Step: 12 Loss: 0.6615071296691895 Accuracy: 0.8125\n",
      "Epoch :  24\n",
      "Step: 0 Loss: 0.428391695022583 Accuracy: 0.8125\n",
      "Step: 4 Loss: 0.8554978370666504 Accuracy: 0.78125\n",
      "Step: 8 Loss: 0.2031775265932083 Accuracy: 0.875\n",
      "Step: 12 Loss: 0.4710421860218048 Accuracy: 0.90625\n",
      "Epoch :  25\n",
      "Step: 0 Loss: 0.35014641284942627 Accuracy: 0.84375\n",
      "Step: 4 Loss: 0.6489889621734619 Accuracy: 0.875\n",
      "Step: 8 Loss: 0.1737874150276184 Accuracy: 0.875\n",
      "Step: 12 Loss: 0.3598438799381256 Accuracy: 0.96875\n",
      "Epoch :  26\n",
      "Step: 0 Loss: 0.3313037157058716 Accuracy: 0.90625\n",
      "Step: 4 Loss: 0.5335779786109924 Accuracy: 0.875\n",
      "Step: 8 Loss: 0.16414129734039307 Accuracy: 0.90625\n",
      "Step: 12 Loss: 0.3193383812904358 Accuracy: 0.96875\n",
      "Epoch :  27\n",
      "Step: 0 Loss: 0.35798221826553345 Accuracy: 0.875\n",
      "Step: 4 Loss: 0.510826587677002 Accuracy: 0.90625\n",
      "Step: 8 Loss: 0.16425961256027222 Accuracy: 0.90625\n",
      "Step: 12 Loss: 0.30245304107666016 Accuracy: 0.96875\n",
      "Epoch :  28\n",
      "Step: 0 Loss: 0.3600353002548218 Accuracy: 0.875\n",
      "Step: 4 Loss: 0.5554497241973877 Accuracy: 0.875\n",
      "Step: 8 Loss: 0.16066697239875793 Accuracy: 0.90625\n",
      "Step: 12 Loss: 0.29770106077194214 Accuracy: 0.96875\n",
      "Epoch :  29\n",
      "Step: 0 Loss: 0.3292015790939331 Accuracy: 0.90625\n",
      "Step: 4 Loss: 0.5903743505477905 Accuracy: 0.875\n",
      "Step: 8 Loss: 0.14473378658294678 Accuracy: 0.90625\n",
      "Step: 12 Loss: 0.28126147389411926 Accuracy: 0.96875\n",
      "Epoch :  30\n",
      "Step: 0 Loss: 0.28377673029899597 Accuracy: 0.875\n",
      "Step: 4 Loss: 0.5941067337989807 Accuracy: 0.875\n",
      "Step: 8 Loss: 0.1255435347557068 Accuracy: 0.9375\n",
      "Step: 12 Loss: 0.25640276074409485 Accuracy: 0.96875\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=1e-3)  # tune this\n",
    "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch : \", epoch+1)\n",
    "    for step, ((image_batch1, label_batch1),(image_batch2, label_batch2)) in enumerate(zip(train_data_A,train_data_B)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model_AB([image_batch1, image_batch2])\n",
    "            # loss format is generally: first argument targets, second argument outputs\n",
    "            loss = loss_fn(label_batch1, logits)\n",
    "\n",
    "        variables = model_AB.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        train_acc_metric(label_batch1, logits)\n",
    "        if not step % 4:\n",
    "            print(\"Step: {} Loss: {} Accuracy: {}\".format(step, loss, train_acc_metric.result()))\n",
    "            train_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0391d598-512b-4a2a-9bb9-1551840324ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c14cf487-e71d-4919-a681-8acee181352e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.625\n"
     ]
    }
   ],
   "source": [
    "test_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "for  step, ((image_batch1, label_batch1),(image_batch2, label_batch2)) in enumerate(zip(test_data_A,test_data_B)):\n",
    "    test_acc_metric(label_batch1, model_AB([image_batch1, image_batch2]))\n",
    "print(\"Test acc: {}\".format(test_acc_metric.result()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
