{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5258058e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3488caa-5a30-4876-b352-5ae8a93afcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pylab\n",
    "import dlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io as imageio\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution3D, MaxPooling3D\n",
    "#from keras.utils import multi_gpu_model\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Concatenate, Input, concatenate, add, multiply, maximum, LSTM, Reshape\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ca99e",
   "metadata": {},
   "source": [
    "### GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4776d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_available = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6afb8ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f82c1c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c48a2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading Excel to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e51a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_excel('/Thesis/Datasets/CASME2/CASME2-coding-20190701.xlsx') #index_col=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b384481-f69a-4838-9269-4e141bff4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Thesis/CASME2_aug.xlsx') #index_col=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4ea6d85-0105-483e-963d-6ae07955ad10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subject              510\n",
       "Filename             510\n",
       "Unnamed: 2             0\n",
       "OnsetFrame           510\n",
       "ApexFrame            510\n",
       "OffsetFrame          510\n",
       "Unnamed: 6             0\n",
       "Action Units         510\n",
       "Estimated Emotion    510\n",
       "Number of frames     510\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2cfc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Estimated Emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed366712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>OnsetFrame</th>\n",
       "      <th>ApexFrame</th>\n",
       "      <th>OffsetFrame</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Action Units</th>\n",
       "      <th>Estimated Emotion</th>\n",
       "      <th>Number of frames</th>\n",
       "      <th>Sub_FileName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EP02_01f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>59</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>happiness</td>\n",
       "      <td>41</td>\n",
       "      <td>1_EP02_01f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>EP03_02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131</td>\n",
       "      <td>139</td>\n",
       "      <td>161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>others</td>\n",
       "      <td>31</td>\n",
       "      <td>1_EP03_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>EP04_02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>54</td>\n",
       "      <td>76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>others</td>\n",
       "      <td>56</td>\n",
       "      <td>1_EP04_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>EP04_03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>41</td>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>others</td>\n",
       "      <td>26</td>\n",
       "      <td>1_EP04_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>EP04_04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>49</td>\n",
       "      <td>66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>others</td>\n",
       "      <td>44</td>\n",
       "      <td>1_EP04_04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>26</td>\n",
       "      <td>EP18_46_aug</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>46</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>others</td>\n",
       "      <td>71</td>\n",
       "      <td>26_EP18_46_aug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>26</td>\n",
       "      <td>EP18_47_aug</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>disgust</td>\n",
       "      <td>81</td>\n",
       "      <td>26_EP18_47_aug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>26</td>\n",
       "      <td>EP18_49_aug</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>54</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>disgust</td>\n",
       "      <td>65</td>\n",
       "      <td>26_EP18_49_aug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>26</td>\n",
       "      <td>EP18_50_aug</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>99</td>\n",
       "      <td>161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>disgust</td>\n",
       "      <td>84</td>\n",
       "      <td>26_EP18_50_aug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>26</td>\n",
       "      <td>EP18_51_aug</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>64</td>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4+16</td>\n",
       "      <td>disgust</td>\n",
       "      <td>61</td>\n",
       "      <td>26_EP18_51_aug</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows Ã 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subject     Filename  Unnamed: 2  OnsetFrame  ApexFrame  OffsetFrame  \\\n",
       "0         1     EP02_01f         NaN          46         59           86   \n",
       "1         1      EP03_02         NaN         131        139          161   \n",
       "2         1      EP04_02         NaN          21         54           76   \n",
       "3         1      EP04_03         NaN          31         41           56   \n",
       "4         1      EP04_04         NaN          23         49           66   \n",
       "..      ...          ...         ...         ...        ...          ...   \n",
       "505      26  EP18_46_aug         NaN          31         46          101   \n",
       "506      26  EP18_47_aug         NaN           6         49           86   \n",
       "507      26  EP18_49_aug         NaN          16         54           80   \n",
       "508      26  EP18_50_aug         NaN          78         99          161   \n",
       "509      26  EP18_51_aug         NaN          21         64           81   \n",
       "\n",
       "     Unnamed: 6 Action Units Estimated Emotion  Number of frames  \\\n",
       "0           NaN           12         happiness                41   \n",
       "1           NaN           18            others                31   \n",
       "2           NaN            4            others                56   \n",
       "3           NaN            4            others                26   \n",
       "4           NaN            4            others                44   \n",
       "..          ...          ...               ...               ...   \n",
       "505         NaN           16            others                71   \n",
       "506         NaN            4           disgust                81   \n",
       "507         NaN            4           disgust                65   \n",
       "508         NaN            4           disgust                84   \n",
       "509         NaN         4+16           disgust                61   \n",
       "\n",
       "       Sub_FileName  \n",
       "0        1_EP02_01f  \n",
       "1         1_EP03_02  \n",
       "2         1_EP04_02  \n",
       "3         1_EP04_03  \n",
       "4         1_EP04_04  \n",
       "..              ...  \n",
       "505  26_EP18_46_aug  \n",
       "506  26_EP18_47_aug  \n",
       "507  26_EP18_49_aug  \n",
       "508  26_EP18_50_aug  \n",
       "509  26_EP18_51_aug  \n",
       "\n",
       "[510 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Subject'] = df['Subject'].astype(str)\n",
    "df['Sub_FileName'] = df[['Subject', 'Filename']].apply(lambda x: '_'.join(x), axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b71cab1-a53b-4e81-bb93-5a6961447f01",
   "metadata": {},
   "source": [
    "### Combining from  7 classes to 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c691e3b2-58af-446b-b8bf-928ff0f93ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Sub_FileName  ApexFrame  Number of frames\n",
      "0        1_EP02_01f         59                41\n",
      "15        2_EP09_01         56               100\n",
      "18        2_EP11_01         69                91\n",
      "19        2_EP13_04         51                56\n",
      "20        2_EP14_01        114                46\n",
      "..              ...        ...               ...\n",
      "485  24_EP18_03_aug         76                36\n",
      "490  25_EP10_10_aug         91                98\n",
      "491  25_EP12_01_aug        151                56\n",
      "493  26_EP03_10_aug        149                86\n",
      "500  26_EP13_02_aug         54                85\n",
      "\n",
      "[120 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "positive = df[(df['Estimated Emotion'] == 'happiness' )| (df['Estimated Emotion'] == 'surprise')][['Sub_FileName','ApexFrame','Number of frames']]\n",
    "print(positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c985482f-15e7-4896-b135-0ca450f6ec05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive : 120\n",
      "Negative : 192\n",
      "Neutral : 198\n"
     ]
    }
   ],
   "source": [
    "positive = df[(df['Estimated Emotion'] == 'happiness' )| (df['Estimated Emotion'] == 'surprise')][['Sub_FileName','ApexFrame','Number of frames']]\n",
    "print('Positive :', positive['Sub_FileName'].count())\n",
    "\n",
    "negative = df[(df['Estimated Emotion'] == 'disgust') | (df['Estimated Emotion'] == 'repression') | (df['Estimated Emotion'] == 'fear') | (df['Estimated Emotion'] == 'sadness')][['Sub_FileName','ApexFrame','Number of frames']]\n",
    "print('Negative :',negative['Sub_FileName'].count())\n",
    "\n",
    "neutral = df[df['Estimated Emotion']== 'others'][['Sub_FileName','ApexFrame','Number of frames']]\n",
    "print('Neutral :',neutral['Sub_FileName'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd0d191-0992-476e-b036-4e6abbdf2f97",
   "metadata": {},
   "source": [
    "### Augmentation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2034175-680e-4609-ab5b-2aa36dc572bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_images(subject, vd, img_count, img_flip_lr):\n",
    "    sub = str(subject)\n",
    "    video = str(vd) + '_aug'\n",
    "    main_dir = \"/Thesis/CASME2_RAW_selected/\"\n",
    "    sub_path = os.path.join(main_dir, sub) \n",
    "    os.makedirs(sub_path, exist_ok=True)\n",
    "    \n",
    "    newpath = str(main_dir + sub + '/')\n",
    "    frame_path = os.path.join(newpath, video)\n",
    "    frame_path = str(frame_path)\n",
    "    os.makedirs(frame_path, exist_ok=True)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(frame_path , str(img_count)+'.jpg'), img_flip_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bad3b6-b109-4b27-b72c-83167f825445",
   "metadata": {},
   "source": [
    "### Creating Augmented data in the folders of Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f335963-1abf-44f9-95d9-1f119cacc0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelpath =  '/Thesis/CASME2_RAW_selected/'\n",
    "directorylisting = os.listdir(labelpath)\n",
    "for subject in directorylisting:\n",
    "    count = 0\n",
    "    videopath = labelpath + subject + '/'\n",
    "    Num_videos = os.listdir(videopath)\n",
    "    for vd in Num_videos:\n",
    "        framepath = videopath + vd\n",
    "        framelisting = os.listdir(framepath)\n",
    "        count = count+1\n",
    "        img_count = 0\n",
    "        frames_len = len(framelisting)\n",
    "        framerange = [x  for x in range(frames_len)]\n",
    "        for frame in framerange:\n",
    "            img_count = img_count + 1\n",
    "            imagepath = framepath + '/' + framelisting[frame]\n",
    "            image = cv2.imread(imagepath)\n",
    "            img_flip_lr = cv2.flip(image, 1)\n",
    "            #aug_images(subject, vd, img_count, img_flip_lr)\n",
    "        #print('subject : ', subject, '  Videos count : ',count, '  Frames count : ',img_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189a2a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pre-processing of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0051297-e42a-468a-9955-ea7d9a2afeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_casme_list = []\n",
    "onset_offset_frames = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "336ad162-ea11-4176-a5a2-6bbb2495f5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [00:14,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192it [00:22,  8.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198it [00:22,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for lab in (positive['Sub_FileName'], negative['Sub_FileName'], neutral['Sub_FileName']): \n",
    "    subDirectory = lab#.tolist() \n",
    "    num = num + 1\n",
    "    i = 0\n",
    "    count = 0\n",
    "    for i, sub in tqdm(enumerate(subDirectory)):\n",
    "        sub = str(sub).split('_')\n",
    "        labelpath = '/Thesis/Datasets/CASME2/CASME2_RAW_selected/'+'sub'+ sub[0].zfill(2) +'/'\n",
    "        directorylisting = os.listdir(labelpath)      \n",
    "        for video in directorylisting:\n",
    "            if video == str(sub[1]+'_'+sub[2]):\n",
    "                #frames = []\n",
    "                total_face_frames = []\n",
    "                if num == 1:\n",
    "                    ApexFrame = positive['ApexFrame'].iloc[i]\n",
    "                    ApexImage = 'img'+ str(ApexFrame) + '.jpg'\n",
    "                elif num == 2:\n",
    "                    ApexFrame = negative['ApexFrame'].iloc[i]\n",
    "                    ApexImage = 'img'+ str(ApexFrame) + '.jpg'\n",
    "                else:\n",
    "                    ApexFrame = neutral['ApexFrame'].iloc[i]\n",
    "                    ApexImage = 'img'+ str(ApexFrame) + '.jpg'\n",
    "                videopath = labelpath + video\n",
    "                framelisting = os.listdir(videopath)\n",
    "                for index, value in enumerate(framelisting):\n",
    "                    frames = []\n",
    "                    if str(value) == str(ApexImage):\n",
    "                        start = index - onset_offset_frames\n",
    "                        end = index + onset_offset_frames\n",
    "                        if (len(framelisting) > end) & (start > 0):\n",
    "                            # print('Subject : ', sub[0].zfill(2), ' video : ', video, ' ApexFrame : ', ApexFrame, '  Apex_Image : ', index, ' start : ', start, ' end : ', end )\n",
    "                            framerange = [x  for x in range(start, end)]\n",
    "                            count = count + 1\n",
    "                            for frame in framerange:\n",
    "                                imagepath = videopath + \"/\" + framelisting[frame]\n",
    "                                image = cv2.imread(imagepath)\n",
    "                                imageresize = cv2.resize(image, (64, 64), interpolation = cv2.INTER_AREA)\n",
    "                                grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)\n",
    "                                image_dim = np.expand_dims(grayimage, axis=2)\n",
    "                                frames.append(image_dim)\n",
    "                            frames = np.asarray(frames)\n",
    "                            training_casme_list.append(frames)\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a46a799f-c10e-4ddb-8b3b-332deb621ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_casme_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1471b156-aa00-4877-88ca-37cc53b4fa1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 64, 64, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_casme_list[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ee108b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "110+164"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44adb646",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating Target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "379871d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_casme_list = np.asarray(training_casme_list)\n",
    "training_casme_samples = len(training_casme_list)\n",
    "\n",
    "training_casme_labels = np.zeros((training_casme_samples, ), dtype = int)\n",
    "\n",
    "training_casme_labels[0:110] = 0\n",
    "training_casme_labels[110:274] = 1\n",
    "training_casme_labels[274:440] = 2\n",
    "\n",
    "training_casme_labels = np_utils.to_categorical(training_casme_labels, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ef4745f-6e6d-4000-b378-abfa73689d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_casme_data = [training_casme_list, training_casme_labels]\n",
    "(training_casme_set, traininglabels_casme) = (training_casme_data[0], training_casme_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bbcd11f-857c-411e-8ef2-4f42e4c84ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_casme_set = training_casme_set.astype('float32')\n",
    "training_casme_set -= np.mean(training_casme_set)\n",
    "training_casme_set /= np.max(training_casme_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b048a165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 16, 64, 64, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_casme_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed54da",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Save Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1edc8d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Nump Arrays to save time\n",
    "np.save('/Thesis/Multi_Scale_MicroExp/Sample_datasets/microexp_casme_images.npy', training_casme_set)\n",
    "np.save('/Thesis/Multi_Scale_MicroExp/Sample_datasets/microexp_casme_labels.npy', training_casme_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6e89",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Loading Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_casme_set = None\n",
    "training_casme_labels = None\n",
    "training_casme_set  = np.load('/Thesis/Multi_Scale_MicroExp/Sample_datasets/microexp_casme_images.npy')\n",
    "training_casme_labels = np.load('/Thesis/Multi_Scale_MicroExp/Sample_datasets/microexp_casme_labels.npy')\n",
    "\n",
    "training_casme_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee2992-0b6b-4542-bf33-32e6ab8d9c1c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e177ced-d4eb-4004-8fbf-aa1609d1fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution3D(16, (3, 3, 3), input_shape=(16, 64, 64, 1), activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'SGD', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd97a8-872f-4d11-98d4-62d810658b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained weights\n",
    "\n",
    "#model.load_weights('/Thesis/MicroExpSTCNN/Training_dataset/weights_microexpstcnn/weights-improvement-53-0.88.hdf5')\n",
    "\n",
    "#model.load_weights('/Thesis/MicroExpSTCNN/CASME_SQUARE/weights-improvement-53-0.88.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e86627-9d4b-4460-a0df-856c6dfa148b",
   "metadata": {},
   "source": [
    "#### Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760bf393-b6a8-47ac-9298-e42b9155e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=10, decay_rate=0.90, staircase=True)\n",
    "\n",
    "model.compile( loss=\"categorical_crossentropy\", optimizer= tf.keras.optimizers.Adam(learning_rate=lr_schedule), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db381c68-91cc-4952-ab00-b3bfc56c9e2a",
   "metadata": {},
   "source": [
    "#### Creating checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf22e16a-8566-41f6-acc1-de5461aa377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"/Thesis/Multi_Scale_MicroExp/Sample_datasets/weights_microexp/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=150)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ebf60-8672-496d-8826-d2aeffb3613f",
   "metadata": {},
   "source": [
    "#### Spliting the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f58faf-02e2-4c4e-a119-0237d4a01c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, validation_images, train_labels, validation_labels =  train_test_split(training_casme_set, training_casme_labels, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eeb2e1-108d-4fca-b6cc-9ffced1640ba",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd79389-9297-4c36-ab36-be7aac19f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_images, train_labels, validation_data = (validation_images, validation_labels), callbacks=callbacks_list, batch_size = 8, epochs = 300, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e8e6d1-f5bc-499e-a418-ed1050cd422e",
   "metadata": {},
   "source": [
    "#### Finding Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe846821-c995-4317-8ada-e07ae2e2cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(validation_images)\n",
    "predictions_labels = np.argmax(predictions, axis=1)\n",
    "validation_labels = np.argmax(validation_labels, axis=1)\n",
    "cfm = confusion_matrix(validation_labels, predictions_labels)\n",
    "print (cfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca461354-8c14-457a-a5f1-4b3bddf96c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
